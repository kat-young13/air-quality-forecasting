{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "* Read in Data\n",
    "* Get averages by day\n",
    "* Combine the files with the particle sizes with the files with environmental factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bme_file_location = \"sofia_small/*bme280sof.csv\"\n",
    "sds_file_location = \"sofia_small/*sds011sof.csv\"\n",
    "\n",
    "file_type = \"csv\"\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "df_bme = spark.read.format(file_type) \\\n",
    "    .option(\"inferSchema\", infer_schema) \\\n",
    "    .option(\"header\", first_row_is_header) \\\n",
    "    .option(\"sep\", delimiter) \\\n",
    "    .load(bme_file_location)\n",
    "\n",
    "df_sds = spark.read.format(file_type) \\\n",
    "    .option(\"inferSchema\", infer_schema) \\\n",
    "    .option(\"header\", first_row_is_header) \\\n",
    "    .option(\"sep\", delimiter) \\\n",
    "    .load(sds_file_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+\n",
      "|        ts|           avg(P1)|           avg(P2)|\n",
      "+----------+------------------+------------------+\n",
      "|2017-07-01|17.764459663706905| 8.341274009698298|\n",
      "|2017-07-02| 9.846284524930946| 6.325375406399083|\n",
      "|2017-07-03| 20.35557791635185|17.195223293020778|\n",
      "|2017-07-04| 8.984114511906204| 6.868896334621589|\n",
      "|2017-07-05|10.412705222705204| 7.964031059031034|\n",
      "|2017-07-06| 10.85810864999049| 8.447780535930221|\n",
      "|2017-07-07| 9.614079073024804| 7.430200547526521|\n",
      "|2017-07-08| 12.10184730986929| 9.885236809576535|\n",
      "|2017-07-09|12.441132935466957|10.319859653725107|\n",
      "|2017-07-10|14.278580865387667|12.425794746989531|\n",
      "|2017-07-11|16.458481004748865|13.907630592351836|\n",
      "|2017-07-12|14.077904752827688|10.800456856017346|\n",
      "|2017-07-13| 11.50965046888325| 8.878007956805918|\n",
      "|2017-07-14| 5.461827450735781|  3.10989585931652|\n",
      "|2017-07-15|10.245437171815821| 7.799760959824183|\n",
      "|2017-07-16|11.484685678666041|  9.46174505220515|\n",
      "|2017-07-17| 8.730244358596998|  7.05922492028453|\n",
      "|2017-07-18|11.758467153284702| 9.619662928016169|\n",
      "|2017-07-19|10.757269963337261| 8.447159080747532|\n",
      "|2017-07-20|10.377995553018224| 8.262260148432995|\n",
      "+----------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import to_timestamp,date_format\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import count, avg\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "df_sds_transformed = df_sds.withColumn('year',year(df_sds.timestamp))\\\n",
    "    .withColumn('month', month(df_sds.timestamp))\\\n",
    "    .withColumn(\"day\", date_format(col(\"timestamp\"), \"d\"))\\\n",
    "    .withColumn(\"ts\", to_date(col(\"timestamp\")).cast(\"date\"))\n",
    "\n",
    "df_sds_transformed = df_sds_transformed.groupBy(\"ts\").agg(avg(\"P1\"), avg(\"P2\")).orderBy([\"ts\"], ascending=True)\n",
    "\n",
    "df_sds_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+------------------+------------------+\n",
      "|        ts|    avg(pressure)|  avg(temperature)|     avg(humidity)|\n",
      "+----------+-----------------+------------------+------------------+\n",
      "|2017-07-01|94572.18985080464| 33.33327613327619|32.792403355736745|\n",
      "|2017-07-02|94441.42854684066|28.197254514672572| 44.52180304740427|\n",
      "|2017-07-03|94668.76243252479| 18.25461707200767| 78.17694325226547|\n",
      "|2017-07-04|95313.96683276288| 22.32803235375923|  50.4074079911003|\n",
      "|2017-07-05|95440.82530922632|23.534423652694652|44.841247660928104|\n",
      "|2017-07-06|95312.02019876736|25.778363851992424| 42.49701185958226|\n",
      "|2017-07-07|95248.96706425186|27.469182004089852|40.482749797878675|\n",
      "|2017-07-08|95059.96317789162|  25.7144688644688| 51.47889969005336|\n",
      "|2017-07-09|95089.78527820377|27.075451422027033| 49.46747614048477|\n",
      "|2017-07-10| 95128.1010232264|28.758966410703227|44.910974230932034|\n",
      "|2017-07-11|95059.89666140139|30.580405242122854| 41.59478715493988|\n",
      "|2017-07-12|94791.26359009359| 30.41663144117502| 36.67443695768059|\n",
      "|2017-07-13| 94660.5128211271|28.239342143762816|  40.2070233102511|\n",
      "|2017-07-14|94698.49576498086|25.985152705061278| 35.94933100639909|\n",
      "|2017-07-15|94722.05143758388|23.879997175673175| 48.98377471286015|\n",
      "|2017-07-16|95101.30591753831|17.941519693602615| 71.34576150172559|\n",
      "|2017-07-17|95316.44255811958|18.957777725805787| 65.99736470368151|\n",
      "|2017-07-18|95171.17375061983|22.368150564730172| 55.49259010934609|\n",
      "|2017-07-19|94549.30462410931|24.076660417047684| 47.53020578013586|\n",
      "|2017-07-20|94961.17755593521| 25.72600174937875| 43.76265629315902|\n",
      "+----------+-----------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bme_transformed = df_bme.withColumn('year',year(df_bme.timestamp))\\\n",
    "    .withColumn('month', month(df_bme.timestamp))\\\n",
    "    .withColumn(\"day\", date_format(col(\"timestamp\"), \"d\"))\\\n",
    "    .withColumn(\"ts\", to_date(col(\"timestamp\")).cast(\"date\"))\n",
    "\n",
    "df_bme_transformed = df_bme_transformed.groupBy(\"ts\").agg(avg(\"pressure\"), avg(\"temperature\"), avg(\"humidity\"))\\\n",
    "    .orderBy([\"ts\"], ascending=True)\n",
    "\n",
    "df_bme_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|        ts|    avg(pressure)|  avg(temperature)|     avg(humidity)|           avg(P1)|           avg(P2)|\n",
      "+----------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|2017-07-01|94572.18985080464| 33.33327613327619|32.792403355736745|17.764459663706905| 8.341274009698298|\n",
      "|2017-07-02|94441.42854684066|28.197254514672572| 44.52180304740427| 9.846284524930946| 6.325375406399083|\n",
      "|2017-07-03|94668.76243252479| 18.25461707200767| 78.17694325226547| 20.35557791635185|17.195223293020778|\n",
      "|2017-07-04|95313.96683276288| 22.32803235375923|  50.4074079911003| 8.984114511906204| 6.868896334621589|\n",
      "|2017-07-05|95440.82530922632|23.534423652694652|44.841247660928104|10.412705222705204| 7.964031059031034|\n",
      "|2017-07-06|95312.02019876736|25.778363851992424| 42.49701185958226| 10.85810864999049| 8.447780535930221|\n",
      "|2017-07-07|95248.96706425186|27.469182004089852|40.482749797878675| 9.614079073024804| 7.430200547526521|\n",
      "|2017-07-08|95059.96317789162|  25.7144688644688| 51.47889969005336| 12.10184730986929| 9.885236809576535|\n",
      "|2017-07-09|95089.78527820377|27.075451422027033| 49.46747614048477|12.441132935466957|10.319859653725107|\n",
      "|2017-07-10| 95128.1010232264|28.758966410703227|44.910974230932034|14.278580865387667|12.425794746989531|\n",
      "|2017-07-11|95059.89666140139|30.580405242122854| 41.59478715493988|16.458481004748865|13.907630592351836|\n",
      "|2017-07-12|94791.26359009359| 30.41663144117502| 36.67443695768059|14.077904752827688|10.800456856017346|\n",
      "|2017-07-13| 94660.5128211271|28.239342143762816|  40.2070233102511| 11.50965046888325| 8.878007956805918|\n",
      "|2017-07-14|94698.49576498086|25.985152705061278| 35.94933100639909| 5.461827450735781|  3.10989585931652|\n",
      "|2017-07-15|94722.05143758388|23.879997175673175| 48.98377471286015|10.245437171815821| 7.799760959824183|\n",
      "|2017-07-16|95101.30591753831|17.941519693602615| 71.34576150172559|11.484685678666041|  9.46174505220515|\n",
      "|2017-07-17|95316.44255811958|18.957777725805787| 65.99736470368151| 8.730244358596998|  7.05922492028453|\n",
      "|2017-07-18|95171.17375061983|22.368150564730172| 55.49259010934609|11.758467153284702| 9.619662928016169|\n",
      "|2017-07-19|94549.30462410931|24.076660417047684| 47.53020578013586|10.757269963337261| 8.447159080747532|\n",
      "|2017-07-20|94961.17755593521| 25.72600174937875| 43.76265629315902|10.377995553018224| 8.262260148432995|\n",
      "+----------+-----------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df = df_bme_transformed.join(df_sds_transformed, on=['ts'], how='left').orderBy([\"ts\"], ascending=True)\n",
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NON-TimeSeries: Regression\n",
    "* Used for baseline ideas of how a \"regular\" (non-time-series) would perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols = ['avg(pressure)', 'avg(temperature)', 'avg(humidity)'], outputCol = 'features')\n",
    "features_df = vectorAssembler.transform(combined_df)\n",
    "features_df = features_df.select(['features', 'avg(P2)'])\n",
    "test_features_df = vectorAssembler.transform(x_test)\n",
    "train_features_df = vectorAssembler.transform(x_train)\n",
    "test_features_df = test_features_df.withColumnRenamed(\"avg(P2)\",\"label\")\n",
    "train_features_df = train_features_df.withColumnRenamed(\"avg(P2)\",\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression (pressure, temperature, humidity) - Non Time-Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = 'features', labelCol='label', maxIter=50)\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(lr.regParam, [0.1, 0.3, 0.5]) \\\n",
    "            .addGrid(lr.elasticNetParam, [.5, .7, .9]) \\\n",
    "            .build()\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=param_grid, evaluator=RegressionEvaluator(), numFolds=5).setParallelism(8)\n",
    "cvModel = cv.fit(train_features_df)\n",
    "besty = cvModel.bestModel\n",
    "print(\"  ElasticNetParam:\", besty._java_obj.parent().getElasticNetParam())\n",
    "print(\"  RegParam:\", besty._java_obj.parent().getRegParam())\n",
    "test_predictions = besty.transform(test_features_df)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\")\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "print(RMSE)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"r2\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\")\n",
    "r2 = evaluator.evaluate(test_predictions)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Linear Regression (pressure, temp, humidity) - Non Time-Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "\n",
    "glr = GeneralizedLinearRegression(featuresCol = 'features', labelCol='label', maxIter=50)\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(glr.family, ['gaussian', 'Gamma'])\\\n",
    "            .addGrid(glr.regParam, [0.1, 0.3, 0.5]) \\\n",
    "            .build()\n",
    "cv = CrossValidator(estimator=glr, estimatorParamMaps=param_grid, evaluator=RegressionEvaluator(), numFolds=5).setParallelism(8)\n",
    "\n",
    "cvModel = cv.fit(train_features_df)\n",
    "besty = cvModel.bestModel\n",
    "print(\"  family:\", besty._java_obj.parent().getFamily())\n",
    "print(\"  RegParam:\", besty._java_obj.parent().getRegParam())\n",
    "test_predictions = besty.transform(test_features_df)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\")\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "print(RMSE)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"r2\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\")\n",
    "r2 = evaluator.evaluate(test_predictions)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FMR Regressor (pressure, temp, humidity) - Non Time-Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import FMRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "featureScaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\").fit(train_features_df)\n",
    "fm = FMRegressor(featuresCol=\"scaledFeatures\", stepSize=0.9)\n",
    "pipeline = Pipeline(stages=[featureScaler, fm])\n",
    "model = pipeline.fit(train_features_df)\n",
    "test_predictions = model.transform(test_features_df)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\")\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isotonic Regression (pressure, temp, humidity) - Non Time-Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import IsotonicRegression\n",
    "\n",
    "IR = IsotonicRegression()\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .build()\n",
    "cv = CrossValidator(estimator=IR, estimatorParamMaps=param_grid, evaluator=RegressionEvaluator(), numFolds=5).setParallelism(8)\n",
    "\n",
    "cvModel = cv.fit(train_features_df)\n",
    "besty = cvModel.bestModel\n",
    "test_predictions = besty.transform(test_features_df)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\")\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBT Regressor (pressure, temp, humidity) - Non Time-Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(featuresCol = 'features', labelCol='label', maxIter=50)\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(gbt.maxDepth, [5, 10, 15]) \\\n",
    "            .addGrid(gbt.maxBins, [16]) \\\n",
    "            .build()\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=param_grid, evaluator=RegressionEvaluator(), numFolds=5).setParallelism(8)\n",
    "\n",
    "cvModel = cv.fit(train_features_df)\n",
    "besty = cvModel.bestModel\n",
    "print(\"  max depth:\", besty._java_obj.parent().getMaxDepth())\n",
    "print(\"  max bins:\", besty._java_obj.parent().getMaxBins())\n",
    "test_predictions = besty.transform(test_features_df)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\")\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Prep: Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "def createWindowedDF(window_size, df, features, label):\n",
    "    w = Window.orderBy(\"ts\")\n",
    "    \n",
    "    # delete unnecessary columns\n",
    "    drop_list = []\n",
    "    for col in df.columns:\n",
    "        if col != label and col != \"ts\":\n",
    "            drop_list.append(col)\n",
    "    \n",
    "    feat_names = []\n",
    "    for i in features:\n",
    "        feat_names.append(i[4:-1])\n",
    "        \n",
    "    particle_size = label[4:-1]\n",
    "    \n",
    "    for f in range(0, len(feat_names)):             # for each feature\n",
    "        for i in range(1, window_size+1):           # get all the lags\n",
    "            df = df.withColumn(str(feat_names[f])+\"_lag_\"+str(i), F.lag(F.col(features[f]), i).over(w))\n",
    "    \n",
    "    #df = df.drop('ts')\n",
    "    df = df.na.drop()        # drop all rows with na values\n",
    "    \n",
    "    df = df.drop(*drop_list)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFeatures(df, label):\n",
    "    \n",
    "    features = []\n",
    "    for col in df.columns:\n",
    "        if col != 'ts' and col != label:\n",
    "            features.append(col)\n",
    "    \n",
    "    vectorAssembler = VectorAssembler(inputCols=features, outputCol='features')\n",
    "    \n",
    "    feature_df = vectorAssembler.transform(df)\n",
    "    feature_df = feature_df.select('ts', 'features', label)\n",
    "    feature_df = feature_df.withColumnRenamed(label,\"label\")\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTrainTest(df):\n",
    "    train = df.filter(F.col('ts').between(\"2017-07-01\", \"2019-01-01\"))\n",
    "    test = df.filter(F.col('ts').between(\"2018-10-02\", \"2019-08-01\"))\n",
    "    return train, test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagging Features from past  X days\n",
    "Change the first parameter to decide number of days, the list to change what features to get lags for, and last string should be equal to the column we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|        ts|            features|             label|\n",
      "+----------+--------------------+------------------+\n",
      "|2017-07-04|[78.1769432522654...| 8.984114511906204|\n",
      "|2017-07-05|[50.4074079911003...|10.412705222705204|\n",
      "|2017-07-06|[44.8412476609281...| 10.85810864999049|\n",
      "|2017-07-07|[42.4970118595822...| 9.614079073024804|\n",
      "|2017-07-08|[40.4827497978786...| 12.10184730986929|\n",
      "|2017-07-09|[51.4788996900533...|12.441132935466957|\n",
      "|2017-07-10|[49.4674761404847...|14.278580865387667|\n",
      "|2017-07-11|[44.9109742309320...|16.458481004748865|\n",
      "|2017-07-12|[41.5947871549398...|14.077904752827688|\n",
      "|2017-07-13|[36.6744369576805...| 11.50965046888325|\n",
      "|2017-07-14|[40.2070233102511...| 5.461827450735781|\n",
      "|2017-07-15|[35.9493310063990...|10.245437171815821|\n",
      "|2017-07-16|[48.9837747128601...|11.484685678666041|\n",
      "|2017-07-17|[71.3457615017255...| 8.730244358596998|\n",
      "|2017-07-18|[65.9973647036815...|11.758467153284702|\n",
      "|2017-07-19|[55.4925901093460...|10.757269963337261|\n",
      "|2017-07-20|[47.5302057801358...|10.377995553018224|\n",
      "|2017-07-21|[43.7626562931590...|12.736653805057664|\n",
      "|2017-07-22|[41.4574606175173...| 16.31782252094864|\n",
      "|2017-07-23|[39.8953849979657...| 16.29555268716271|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+--------------------+------------------+\n",
      "|        ts|            features|             label|\n",
      "+----------+--------------------+------------------+\n",
      "|2019-06-01|[43.4526964569776...| 24.97850066326876|\n",
      "|2019-06-02|[65.5818605991107...|22.121130146561452|\n",
      "|2019-06-03|[81.9860119663755...| 16.44211035862256|\n",
      "|2019-06-04|[77.7700467268574...| 22.96557754969085|\n",
      "|2019-06-05|[75.9858753621885...|20.028361659287604|\n",
      "|2019-06-06|[75.9327163645788...|20.961364585110807|\n",
      "|2019-06-07|[73.2294376916642...|21.633451102247086|\n",
      "|2019-06-08|[59.4197510366395...| 24.35988692539965|\n",
      "|2019-06-09|[54.0013525819373...| 21.76117800727091|\n",
      "|2019-06-10|[58.6356227324236...| 18.90690677624381|\n",
      "|2019-06-11|[68.6258744369423...|18.000890456837176|\n",
      "|2019-06-12|[72.4018668848512...|19.673033681882256|\n",
      "|2019-06-13|[60.0352556787952...| 22.31245086420766|\n",
      "|2019-06-14|[51.7037131736714...|22.454018992572525|\n",
      "|2019-06-15|[55.4971645662213...| 21.85477116507943|\n",
      "|2019-06-16|[54.8249974256581...|19.535112897536738|\n",
      "|2019-06-17|[55.0779121516476...| 21.09822180561713|\n",
      "|2019-06-18|[63.6126040560986...|23.252742513648784|\n",
      "|2019-06-19|[65.3026567294304...| 19.54484111335693|\n",
      "|2019-06-20|[70.2016471903885...|21.969973554544055|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Prep\n",
    "window_size = 3\n",
    "feature_list = [\"avg(humidity)\", \"avg(temperature)\", \"avg(pressure)\"]\n",
    "label = \"avg(P1)\"\n",
    "\n",
    "df = createWindowedDF(window_size, combined_df, feature_list, label)\n",
    "feature_df = createFeatures(df, \"avg(P1)\")\n",
    "train, test = splitTrainTest(feature_df)\n",
    "train.show()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ElasticNetParam: 0.9\n",
      "  RegParam: 0.5\n",
      "4.489945733073979\n",
      "-1.3483381028421895\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol = 'features', labelCol='label', maxIter=50)\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(lr.regParam, [0.1, 0.3, 0.5]) \\\n",
    "            .addGrid(lr.elasticNetParam, [.5, .7, .9]) \\\n",
    "            .build()\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=param_grid, evaluator=RegressionEvaluator(), numFolds=5).setParallelism(8)\n",
    "\n",
    "cvModel = cv.fit(train)\n",
    "besty = cvModel.bestModel\n",
    "print(\"  ElasticNetParam:\", besty._java_obj.parent().getElasticNetParam())\n",
    "print(\"  RegParam:\", besty._java_obj.parent().getRegParam())\n",
    "test_predictions = besty.transform(test)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\")\n",
    "lr_RMSE = evaluator.evaluate(test_predictions)\n",
    "print(lr_RMSE)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"r2\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\")\n",
    "r2 = evaluator.evaluate(test_predictions)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glr = GeneralizedLinearRegression(featuresCol = 'features', labelCol='label', maxIter=50)\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(glr.family, ['gaussian', 'Gamma'])\\\n",
    "            .addGrid(glr.regParam, [0.1, 0.3, 0.5]) \\\n",
    "            .build()\n",
    "cv = CrossValidator(estimator=glr, estimatorParamMaps=param_grid, evaluator=RegressionEvaluator(), numFolds=5).setParallelism(8)\n",
    "\n",
    "cvModel = cv.fit(train)\n",
    "besty = cvModel.bestModel\n",
    "print(\"  family:\", besty._java_obj.parent().getFamily())\n",
    "print(\"  RegParam:\", besty._java_obj.parent().getRegParam())\n",
    "test_predictions = besty.transform(test)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\")\n",
    "glr_RMSE = evaluator.evaluate(test_predictions)\n",
    "print(glr_RMSE)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"r2\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\")\n",
    "r2 = evaluator.evaluate(test_predictions)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(featuresCol = 'features', labelCol='label', maxIter=50)\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(gbt.maxDepth, [5, 10, 15]) \\\n",
    "            .addGrid(gbt.maxBins, [16]) \\\n",
    "            .build()\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=param_grid, evaluator=RegressionEvaluator(), numFolds=5).setParallelism(8)\n",
    "\n",
    "cvModel = cv.fit(train)\n",
    "besty = cvModel.bestModel\n",
    "print(\"  max depth:\", besty._java_obj.parent().getMaxDepth())\n",
    "print(\"  max bins:\", besty._java_obj.parent().getMaxBins())\n",
    "test_predictions = besty.transform(test)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\")\n",
    "gbt_RMSE = evaluator.evaluate(test_predictions)\n",
    "print(gbt_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import IsotonicRegression\n",
    "IR = IsotonicRegression()\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .build()\n",
    "cv = CrossValidator(estimator=IR, estimatorParamMaps=param_grid, evaluator=RegressionEvaluator(), numFolds=5).setParallelism(8)\n",
    "\n",
    "cvModel = cv.fit(train)\n",
    "besty = cvModel.bestModel\n",
    "test_predictions = besty.transform(test)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\")\n",
    "ir_RMSE = evaluator.evaluate(test_predictions)\n",
    "print(ir_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*** RMSE Values ***\")\n",
    "print(\"Linear Regression: \" + str(lr_RMSE))\n",
    "print(\"Generalized Linear Regression\" + str(glr_RMSE))\n",
    "print(\"GBT Regression: \" + str(gbt_RMSE))\n",
    "print(\"Isotonic Regression: \" + str(ir_RMSE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
